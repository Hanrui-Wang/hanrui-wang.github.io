<!doctype html>
<html>

<head>
  <title> MicroNet for Efficient Language Modeling</title>
  <meta name="viewport" content="width=device-width,initial-scale=1">
  <link href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-alpha.6/css/bootstrap.min.css" rel="stylesheet"
    integrity="sha384-rwoIResjU2yc3z8GV/NPeZWAv56rSmLldC3R/AZzGRnGxQQKnKkoFVhFQhNUwEyJ" crossorigin="anonymous">
  <script src="https://code.jquery.com/jquery-3.2.1.min.js"
    integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous"></script>
  <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,700" rel="stylesheet">
  <link href="style.css" rel="stylesheet">
  <link href="style_extra.css" rel="stylesheet">
</head>

<body class="nd-docs">
  <div class="nd-pageheader">
    <div class="container">
      <p class="lead" style="font-size:300%;">
        MicroNet for Efficient Language Modeling
        <address>
                    <nobr>
            <a href="https://github.com/ZhongxiaYan">Zhongxia Yan</a>
            <sup>1</sup>
            ,
          </nobr>
                    <nobr>
            <a href="https://hanruiwang.me">Hanrui Wang</a>
            <sup>1</sup>
            ,
          </nobr>
                    <nobr>
            <a href="http://demiguo.me/">Demi Guo</a>
            <sup>1</sup>
            ,
          </nobr>
                    <nobr>
            <a href="https://songhan.mit.edu">Song Han</a>
            <sup>1</sup>
            
          </nobr>
                    <br>
                    <nobr><sup>1</sup>Massachusetts Institute of Technology,</nobr>
                    <nobr><sup>2</sup>Harvard University</nobr>
                  </address>
      </p>
    </div>
  </div> <!-- end nd-pageheader -->

  <div class="container">

    <div class="row">
      <div class="col text-center">
        <p>
                    <a href="https://arxiv.org/abs/2005.07877" class="d-inline-block p-3 align-top">
            <img height="100" width="78" src="assets/paper_thumbnail.png" style="border:1px solid"
              data-nothumb><br>
            JMLR 2020 Paper
          </a>
                    <a href="https://github.com/mit-han-lab/neurips-micronet" class="d-inline-block p-3 align-top">
            <img height="100" width="170" src="assets/code.png" style="border:1px solid"
              data-nothumb><br>
            Code
          </a>
                    <a href="https://slideslive.com/38923176/micronet-challenge" class="d-inline-block p-3 align-top">
            <img height="100" width="172" src="assets/talk.png" style="border:1px solid"
              data-nothumb><br>
            Talk (Starts from 26:17)
          </a>
                </div>
    </div>

    <div class="row">
      <div class="col">

                
                <p><p>It is important to design compact language models for efficient deployment. We improve upon recent advances in both the language modeling domain and the model-compression domain to construct parameter and computation efficient language models. We use an efficient transformer-based architecture with adaptive embedding and softmax, differentiable non-parametric cache, Hebbian softmax, knowledge distillation, network pruning, and low-bit quantization. In this paper, we provide the winning solution to the NeurIPS 2019 MicroNet Challenge in the language modeling track. Compared to the baseline language model provided by the MicroNet Challenge, our model is 90 times more parameter-efficient and 36 times more computation-efficient while achieving the required test perplexity of 35 on the Wikitext-103 dataset. We hope that this work will aid future research into efficient language models, and we have released our full source code at https://github.com/mit-han-lab/neurips-micronet.</p></p>
        
                                <h2>Overview of Techniques</h2>
        
                <p><p align="center">
    <img src="assets/overview.png" width="50%">
</p></p>
        
                                <h2>Citation</h2>
        
        
                <pre class="highlight"> @inproceedings{yan2020micronet,
    title     = {MicroNet for Efficient Language Modeling},
    author    = {Yan, Zhongxia and Wang, Hanrui and Guo, Demi and Liu, Zhijian and Han, Song},
    booktitle = {Journal of Machine Learning Research},
    year      = {2020}
} </pre>
                        
                <p><p><strong>Acknowledgments</strong>: We sincerely thank Facebook Faculty Award, AWS Machine Learning Award, and AMD for sponsoring this research. We are grateful to Phillip Isola for helpful discussions.</p></p>
        
                
      </div>
    </div> <!-- row -->

  </div> <!-- container -->

  <script>
    var coll = document.getElementsByClassName("collapsible");
    var i;

    for (i = 0; i < coll.length; i++) {
      coll[i].addEventListener("click", function () {
        this.classList.toggle("active");
        var content = this.nextElementSibling;
        if (content.style.maxHeight) {
          content.style.maxHeight = null;
        } else {
          content.style.maxHeight = content.scrollHeight * 50 + "px";
        }
        content.style.height = "550%";
      });
    }
  </script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        processEscapes: true
      }
    });
  </script>
  
  <script type="text/javascript"
      src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>
</body>

</html>